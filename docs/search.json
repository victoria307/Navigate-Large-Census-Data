[
  {
    "objectID": "an_01.html",
    "href": "an_01.html",
    "title": "Data Analysis (an_01)",
    "section": "",
    "text": "For this file, your goal is to:\n\nRun the most fundamental analysis that supports your core research question\nSave the regression results in LaTeX format for easy integration into your report or paper"
  },
  {
    "objectID": "an_01.html#purpose",
    "href": "an_01.html#purpose",
    "title": "Data Analysis (an_01)",
    "section": "",
    "text": "For this file, your goal is to:\n\nRun the most fundamental analysis that supports your core research question\nSave the regression results in LaTeX format for easy integration into your report or paper"
  },
  {
    "objectID": "an_01.html#construct-the-regression-formula",
    "href": "an_01.html#construct-the-regression-formula",
    "title": "Data Analysis (an_01)",
    "section": "2 Construct the Regression Formula",
    "text": "2 Construct the Regression Formula\nüìå The first thing you‚Äôll want to do is set up a list of variables you‚Äôll use in your analysis. That way, if you ever want to make changes, you only need to update it in one spot. We usually define this list as othervar near the bottom of cl_02. It‚Äôs a simple trick that makes your code more flexible‚Äîkind of like a ‚Äúforward-looking‚Äù move, as economists might say.\n\ndata_2000 &lt;- read_csv(\"data/source/your cl_03 result.csv\")\n\nvarlist &lt;- c(\"cov_1\", \"cov_2\", \"cov_3\", ...)\n\nAfter you setting up this, you can start to write up your model\n\nformula_1 &lt;- paste(\"dep_var1 ~ ind_var1 + ind_var2 +\", varlist)\n\nmodel_1 &lt;- lm(formula_1, data = data_2000)\n\nformula_2 &lt;- paste(\"dep_var2 ~ ind_var1 + ind_var2 +\", varlist)\n\nmodel_2 &lt;- lm(formula_2, data = data_2000)\n\nüìå The models I‚Äôve written here are just examples‚Äîyou‚Äôre encouraged to define your own. You can use different variables, try alternative specifications, or even work with different datasets. The key is to present your comparisons clearly, so your audience can easily understand the point you‚Äôre trying to make."
  },
  {
    "objectID": "an_01.html#generate-the-table",
    "href": "an_01.html#generate-the-table",
    "title": "Data Analysis (an_01)",
    "section": "3 Generate the table",
    "text": "3 Generate the table\n\nstargazer(model_1, model_2,\n          type = \"text\",  \n          # my personal habit is to store the table in text file\n          title = \"Basic Analysis\",\n          align = TRUE,\n          out = \"the name you assigned to the file of table\")\n\nüìå you can refer this guild in exploring how you can use this function to produce the tables."
  },
  {
    "objectID": "cl_00.html",
    "href": "cl_00.html",
    "title": "Data Extraction and Renaming (cl_00)",
    "section": "",
    "text": "In this step, we load the raw dataset, convert it into a more flexible format, and rename it to make it easier to reference in later steps. Think of this as preparing your workspace before you start cleaning."
  },
  {
    "objectID": "cl_00.html#purpose",
    "href": "cl_00.html#purpose",
    "title": "Data Extraction and Renaming (cl_00)",
    "section": "",
    "text": "In this step, we load the raw dataset, convert it into a more flexible format, and rename it to make it easier to reference in later steps. Think of this as preparing your workspace before you start cleaning."
  },
  {
    "objectID": "cl_00.html#setup",
    "href": "cl_00.html#setup",
    "title": "Data Extraction and Renaming (cl_00)",
    "section": "2 Setup",
    "text": "2 Setup\nWe‚Äôll start by loading the required libraries for reading Stata, Excel, and csv files:\n\n# Load required libraries\nlibrary(haven)      # for reading/writing Stata files\nlibrary(openxlsx)   # for reading/writing xlsx files\nlibrary(readr)      # for reading/writing csv files\nlibrary(data.table) # good for large csv files\n\n\n2.1 Load the data and rename it\nMost datasets are available as .csv files, which we can use directly in R.\nüìå While R supports both .csv and .xlsx formats, .csv is generally preferred because it‚Äôs lightweight, universally compatible, and easier to integrate with other tools and platforms1.\n\n# Load the source IPUMS data file; for example, if I am working with data in 2000, I will directly name the file as 2000 or 2000_acs\ndata_2000 &lt;- read_csv(\"data/source/your_file_name_here.csv\")\n\n\nwrite_csv(data_2000, \"data/outcome/2000.csv\")\n\nIf the files are large, you can also choose to use fread\n\ndata_2000 &lt;- fread(\"data/source/your_file_name_here.csv\")\n\n\nfwrite(data_2000, \"data/outcome/2000.csv\")\n\nIf you have a list of files (I use BLS data as an example here), you can\n\nyears &lt;- c(2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)\n\n# Loop through each year\nfor(year in years) {\n  \n  # Define file paths\n  input_file &lt;- paste0(\"data/source/QCEW/fast_food sector/\", year, \"_Limited-service restaurants.csv\")  #you will be list the name as the way they downloaded on your local\n  output_file &lt;- paste0(\"data/outcome/\", year, \"_bls.csv\")\n  \n  # Read CSV file\n  data &lt;- read_csv(input_file)\n  \n  # Save as CSV file with new name\n  write_csv(data, output_file)\n}"
  },
  {
    "objectID": "cl_00.html#footnotes",
    "href": "cl_00.html#footnotes",
    "title": "Data Extraction and Renaming (cl_00)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a comparison of .csv vs .xlsx in data workflows, see this LinkedIn article: CSV vs Excel: Pros and Cons‚Ü©Ô∏é"
  },
  {
    "objectID": "cl_01.html",
    "href": "cl_01.html",
    "title": "Clean Allocated Values / Combine the datasets (cl_01)",
    "section": "",
    "text": "For this file, your goal is: 1. Remove allocated values that are not meaningful for further analysis 2. Merge or split datasets as needed to prepare for the next stage of cleaning and transformation"
  },
  {
    "objectID": "cl_01.html#purpose",
    "href": "cl_01.html#purpose",
    "title": "Clean Allocated Values / Combine the datasets (cl_01)",
    "section": "",
    "text": "For this file, your goal is: 1. Remove allocated values that are not meaningful for further analysis 2. Merge or split datasets as needed to prepare for the next stage of cleaning and transformation"
  },
  {
    "objectID": "cl_01.html#clean-up-all-allocated-values",
    "href": "cl_01.html#clean-up-all-allocated-values",
    "title": "Clean Allocated Values / Combine the datasets (cl_01)",
    "section": "2 Clean up all allocated values",
    "text": "2 Clean up all allocated values\n\n# Load data\ndata_2000 &lt;- read_csv(\"data/outcome/2000.csv\")\n\n# Define quality variable list\nqvarlist &lt;- c(\"var_1\", \"var_2\"...)\n\n# Remove observations where quality flags are greater than 3 (this is your to-go), then drop the quality variables\n# Note: Definitions for allocated values may vary by variable, so greater than 3 may not be universal, so always double-check the documentation.\n\nfor(qvar in qvarlist) {\n  if(qvar %in% names(data_2000)) {\n    data_2000 &lt;- data_2000 %&gt;%\n      filter(!!sym(qvar) &lt;= 3) %&gt;%\n      select(-!!sym(qvar))\n  }\n}\n\n# Save cleaned data\nwrite_csv(data_2000, \"data/outcome/2000_i.csv\")"
  },
  {
    "objectID": "cl_01.html#merging-the-values",
    "href": "cl_01.html#merging-the-values",
    "title": "Clean Allocated Values / Combine the datasets (cl_01)",
    "section": "3 Merging the values",
    "text": "3 Merging the values\nMost of the cases, if you not download your dataset from the customized dataset, you will have a zip of documents, and you may need to manually merge them.\nFor example, in the BLS Quarterly Census of Employment and Wages, when you download, you may have series of datasets by year, so right now\n\nyears &lt;- c(2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)\n\nTo merge the files together in R,there are two ways for your to do this: 1. you can use the bind_rows function, but this approach is only convenient when you don‚Äôt have so much of the files, and the process can be a little bit of messy\n\nqcew_data_2009 &lt;- read_csv(\"data/outcome/2009_qcew.csv\")\nqcew_data_2010 &lt;- read_csv(\"data/outcome/2010_qcew.csv\")\n...\n\nqcew_combined &lt;- bind_rows(qcew_data_2009, qcew_data_2010, ...)\n\n\nWhen you have many of the years, you can setup a list, and directly merge all the files in the list\n\n\nfile_list &lt;- paste0(\"data/outcome/\", years, \"_qcew.csv\")\n\nqcew_combined &lt;- map_dfr(file_list, read_csv)\n\nüìå If you encounter an error during this step, check: (1) whether the file names are correct, and (2) whether the files have matching column structures.\nüí≠ In this section, I demonstrated how to combine separate files into a single dataset. Now, try the opposite‚Äîsplitting a dataset into parts. Also, take a moment to reflect:\nUnder what assumptions is it appropriate to combine datasets?\nWhen might it be better to keep them separate?"
  },
  {
    "objectID": "cl_02.html",
    "href": "cl_02.html",
    "title": "Data Cleaning Example- cl_02",
    "section": "",
    "text": "In this step, you will:\n\nCreate or encode variables that are not directly available in the raw dataset.\nRename variables to make them more descriptive and easier to work with.\nSelect and save only the relevant variables needed for analysis.\nIdentify and recode missing values as NA to ensure compatibility with later cleaning steps.\n\nThis step transforms your raw data into a structured, analysis-ready format."
  },
  {
    "objectID": "cl_02.html#purpose",
    "href": "cl_02.html#purpose",
    "title": "Data Cleaning Example- cl_02",
    "section": "",
    "text": "In this step, you will:\n\nCreate or encode variables that are not directly available in the raw dataset.\nRename variables to make them more descriptive and easier to work with.\nSelect and save only the relevant variables needed for analysis.\nIdentify and recode missing values as NA to ensure compatibility with later cleaning steps.\n\nThis step transforms your raw data into a structured, analysis-ready format."
  },
  {
    "objectID": "cl_02.html#key-code-examples-for-data-cleaning",
    "href": "cl_02.html#key-code-examples-for-data-cleaning",
    "title": "Data Cleaning Example- cl_02",
    "section": "2 üõ†Ô∏è Key Code Examples for Data Cleaning",
    "text": "2 üõ†Ô∏è Key Code Examples for Data Cleaning\n\n2.1 Replace the N/A values with missing values\nFor example, in IPUMS, the INCWAGE variable uses values above 999998 to indicate missing data. During cleaning, we recode those values as NA to ensure proper handling in analysis.\n\ndata_2000_i &lt;- read_csv(\"data/outcome/2000_i.csv\")\n\n  data_2000_i &lt;- data_2000_i %&gt;%\n  mutate(incwage = ifelse(incwage &gt;= 999999, NA, incwage))\n\nwrite_csv(data_2000_i, \"data/outcome/2000_i.csv\")"
  },
  {
    "objectID": "cl_02.html#generate-a-variable-indicating-whether-or-not-the-individuals-wage-is-top-coded",
    "href": "cl_02.html#generate-a-variable-indicating-whether-or-not-the-individuals-wage-is-top-coded",
    "title": "Data Cleaning Example- cl_02",
    "section": "3 Generate a variable indicating whether or not the individual‚Äôs wage is top-coded",
    "text": "3 Generate a variable indicating whether or not the individual‚Äôs wage is top-coded\nSimilarly, check the documentation to identify and correctly handle top-coded values.\n\ndata_2000_i &lt;- read_csv(\"data/outcome/2000_i.csv\")\n\n  data_2000_i &lt;- data_2000_i %&gt;%\n  mutate(topcoded = (incwage &gt;= 175000 & incwage &lt; 999999))\n\nwrite_csv(data_2000_i, \"data/outcome/2000_i.csv\")"
  },
  {
    "objectID": "cl_02.html#adjust-the-wage-for-inflation-using-the-adjustment-factors-provided-by-ipums",
    "href": "cl_02.html#adjust-the-wage-for-inflation-using-the-adjustment-factors-provided-by-ipums",
    "title": "Data Cleaning Example- cl_02",
    "section": "4 Adjust the wage for inflation using the adjustment factors provided by IPUMS",
    "text": "4 Adjust the wage for inflation using the adjustment factors provided by IPUMS\nSince the adjustment factor varies by year, you‚Äôll need to check the appropriate CPI-U multiplier to convert dollar values to constant 1999 dollars. For other datasets, be sure to read the documentation carefully‚Äîsome may already report inflation-adjusted amounts.\n\ndata_2000_i &lt;- read_csv(\"data/outcome/2000_i.csv\")\n\n  data_2000_i &lt;- data_2000_i %&gt;%\n  mutate(incwage = incwage * 1)\n\nwrite_csv(data_2000_i, \"data/outcome/2000_i.csv\")"
  },
  {
    "objectID": "cl_02.html#generate-other-variables",
    "href": "cl_02.html#generate-other-variables",
    "title": "Data Cleaning Example- cl_02",
    "section": "5 Generate Other Variables",
    "text": "5 Generate Other Variables\nüìå To generate consistent variables across multiple files, consider using a loop to automate the process.\nFor example, if I want to generate consistent definition of age group for my research for all the years, I can :\n\n# Define years to process\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\n# Loop through each year\nfor(i in years) {\n  \n  # Load data\n  data_i &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Generate age groups\n  data_i %&gt;%\n    mutate(\n      ageg = case_when(\n        age &gt;= 16 & age &lt;= 19 ~ 1,\n        age &gt;= 20 & age &lt;= 29 ~ 2,  \n        age &gt;= 30 & age &lt;= 39 ~ 3,\n        age &gt;= 40 & age &lt;= 49 ~ 4,\n        age &gt;= 50 & age &lt;= 59 ~ 5,\n        TRUE ~ NA_real_             #When none of above conditions are made\n      )\n    )\n  \n  # Save data\n  write_csv(data_i, paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n}\n\n  # 1: 16-19 years old\n  # 2: 20-29 years old \n  # 3: 30-39 years old\n  # 4: 40-49 years old\n  # 5: 50-59 years old"
  },
  {
    "objectID": "cl_02.html#keep-the-relevant-variables",
    "href": "cl_02.html#keep-the-relevant-variables",
    "title": "Data Cleaning Example- cl_02",
    "section": "6 Keep the relevant variables",
    "text": "6 Keep the relevant variables\n\n  indvar &lt;- c(\"ind_var1\", \"ind_var2\",...)  # your independent variable, what we also called explanatory variable\n  depvar &lt;- c(\"dep_var1\", \"dep_var2\",...)  # your dependent variable\n  othervar &lt;- c(\"cov_1\", \"cov_2\",  ... )   # other covariates\n  pweight &lt;- c(\"perwt\")                     \n\n  all_vars &lt;- c(depvar, indvar, othervar, pweight)\n\n  data_2000_i &lt;- data_2000_i %&gt;%\n    select(all_of(all_vars))\n\nwrite_csv(data_2000_i, \"data/outcome/2000_pre.csv\")\n# we will call the result in this step as year_pre\n\nüìå The PERWT variable indicates how representative each observation is in reflecting the U.S. population. While it may not be available or necessary in every dataset, it‚Äôs important to look for this weighting factor.\nüí≠ Quick Question: What could happen to your analysis or results if you leave out this variable? Take a moment to think about the potential impact. In what situations might you not need to use this variable? üí° hint: take a closer look at the definition of this variable.\nüìå In this section, you may have noticed heavy use of indentation. This is intentional‚Äîusing tabs and consistent indentation is essential in the data cleaning process. It helps organize your code, improves readability, and makes debugging much easier.\nFor comparison, take a look at the two code examples below. Which one feels clearer or easier to read to you?\n\ndata_i %&gt;%\n    mutate(\n      ageg = case_when(\n        age &gt;= 16 & age &lt;= 19 ~ 1,\n        age &gt;= 20 & age &lt;= 29 ~ 2,  \n        age &gt;= 30 & age &lt;= 39 ~ 3,\n        age &gt;= 40 & age &lt;= 49 ~ 4,\n        age &gt;= 50 & age &lt;= 59 ~ 5,\n        TRUE ~ NA_real_             #When none of above conditions are made\n      )\n    )\n\n\ndata_i %&gt;%\nmutate(\nageg = case_when(\nage &gt;= 16 & age &lt;= 19 ~ 1,\nage &gt;= 20 & age &lt;= 29 ~ 2,  \nage &gt;= 30 & age &lt;= 39 ~ 3,\nage &gt;= 40 & age &lt;= 49 ~ 4,\nage &gt;= 50 & age &lt;= 59 ~ 5,\nTRUE ~ NA_real_             #When none of above conditions are made\n)\n)"
  },
  {
    "objectID": "cl_03.html",
    "href": "cl_03.html",
    "title": "Sample Restrictions (cl_03)",
    "section": "",
    "text": "For this file, your goal is: 1. Filter the dataset to include only the observations relevant to your research question 2. Clean missing values by identifying and handling incomplete or unusable entries 3. Save a final, analysis-ready dataset that‚Äôs structured and scoped for your specific study"
  },
  {
    "objectID": "cl_03.html#purpose",
    "href": "cl_03.html#purpose",
    "title": "Sample Restrictions (cl_03)",
    "section": "",
    "text": "For this file, your goal is: 1. Filter the dataset to include only the observations relevant to your research question 2. Clean missing values by identifying and handling incomplete or unusable entries 3. Save a final, analysis-ready dataset that‚Äôs structured and scoped for your specific study"
  },
  {
    "objectID": "cl_03.html#further-limitation",
    "href": "cl_03.html#further-limitation",
    "title": "Sample Restrictions (cl_03)",
    "section": "2 Further limitation",
    "text": "2 Further limitation\n\n# Load the dataset you cleaned \ndata_2000_pre &lt;- read_csv(\"data/outcome/2000_pre.csv\")\n\nApply any sample restrictions relevant to your analysis.\nFor example, a common one is to exclude individuals in group quarters.\nKeep only those living in households.\n\n  data_2000_pre &lt;- data_2000_pre %&gt;%\n    filter(gq %in% c(1, 2, 5))\n\nüìå You may have noticed that there are many variables you didn‚Äôt initially consider, but they turn out to be essential for your analysis. One example is Group Quarters (GQ)‚Äîa detail that can significantly affect interpretation but is easy to overlook.\nüí≠ Why might we want to set a limitation based on group quarters? Can you identify other variables that are often treated as ‚Äúdefault‚Äù controls in empirical analysis?"
  },
  {
    "objectID": "cl_03.html#missing-value-restrictions",
    "href": "cl_03.html#missing-value-restrictions",
    "title": "Sample Restrictions (cl_03)",
    "section": "3 Missing value restrictions",
    "text": "3 Missing value restrictions\n\n# Keep only individuals without missing values in the variables we need later\n\n  data_2000_pre &lt;- data_2000_pre %&gt;%\n    mutate(\n      touse = complete.cases(select(., incwage, schooling, schooling_sp, exp, exp2, \n                                   topcoded, topcoded_uhr, perwt, ageg, race, marr, \n                                   immigrant, region, statefip, origin, wagehr, wagewk))\n    ) %&gt;%\n    filter(touse)  # &lt;- Refined version"
  },
  {
    "objectID": "cl_03.html#save-the-final-dataset-for-analysis",
    "href": "cl_03.html#save-the-final-dataset-for-analysis",
    "title": "Sample Restrictions (cl_03)",
    "section": "4 Save the final dataset for Analysis",
    "text": "4 Save the final dataset for Analysis\n\nwrite_csv(data_2000_pre, \"data/outcome/2000_final.csv\")"
  },
  {
    "objectID": "data_organization.html",
    "href": "data_organization.html",
    "title": "Data Organization",
    "section": "",
    "text": "The _master file serves as the central script that organizes all your data cleaning and analysis steps. In most research projects, you‚Äôll eventually need to submit your code‚Äîhaving a well-structured master file ensures that reviewers (or your future self) can reproduce the results easily by running just one file."
  },
  {
    "objectID": "data_organization.html#overview-of-the-data-cleaning-procedure-the-_master-file",
    "href": "data_organization.html#overview-of-the-data-cleaning-procedure-the-_master-file",
    "title": "Data Organization",
    "section": "",
    "text": "The _master file serves as the central script that organizes all your data cleaning and analysis steps. In most research projects, you‚Äôll eventually need to submit your code‚Äîhaving a well-structured master file ensures that reviewers (or your future self) can reproduce the results easily by running just one file."
  },
  {
    "objectID": "data_organization.html#setup",
    "href": "data_organization.html#setup",
    "title": "Data Organization",
    "section": "2 Setup",
    "text": "2 Setup\nThe Functions of the basic packages\n\n# Data manipulation and transformation\nlibrary(dplyr)      # Grammar of data manipulation - filter, select, mutate, summarize, group_by\n\n# Data import/export\nlibrary(readr)      # Fast and user-friendly reading of rectangular data (CSV, TSV, etc.)\nlibrary(haven)      # Reading and writing data from statistical software (Stata, SPSS, SAS)\n\n# Data reshaping and tidying\nlibrary(tidyr)      # Tools for reshaping data - pivot_longer, pivot_wider, separate, unite\n\n# Data visualization\nlibrary(ggplot2)    # Grammar of graphics for creating complex, layered plots\n\n# Reproducible reporting\nlibrary(knitr)      # Dynamic report generation - combine R code with text\nlibrary(here)       # Portable file path construction relative to project root\nlibrary(quarto)     # Scientific publishing system for documents, presentations, websites\n\n# Display the result of analysis \nlibrary(stragazer)  # Display the regression result clealy in latex format\n\n\n2.1 Set working directory\nWe typically define the working directory in the master file, so it only needs to be set once. This simplifies code submission‚Äîreviewers can run everything without adjusting multiple scripts.\n\n# change this to your local directory\nsetwd(\"/Users/zhaoxiaoxiao/Desktop/2000 Educ_Return_R\")"
  },
  {
    "objectID": "data_organization.html#part-i-data-cleaning-process",
    "href": "data_organization.html#part-i-data-cleaning-process",
    "title": "Data Organization",
    "section": "3 Part I: Data Cleaning Process",
    "text": "3 Part I: Data Cleaning Process\n\n3.1 Step 1: Data Import and Renaming\nRename the dataset using a clear, descriptive name, and convert it to the desired format at this stage.\n\n# Execute data import and renaming script\nquarto_render(\"program/cl_00.qmd\")\n\n\n\n3.2 Step 2: Clean Allocated Values / Combine the datasets\nIn this step, we remove any allocated or imputed values to maintain data quality and transparency.\nIf the dataset does not contain allocated values, this step is used to merge multiple raw files‚Äîfor example, when data comes in separate parts by geography or time.\nüìå Be sure to clearly document the merging logic and assumptions, especially if the source files were not designed to be combined automatically.\n\n# Clean up all allocated values\nquarto_render(\"program/cl_01.qmd\")\n\n\n\n3.3 Step 3: Variable Generation and Transformation\nThis is where most of the data cleaning happens‚Äîyou‚Äôll recode variables, create new ones, and keep only what you need for analysis.\n\n# Generate and keep relevant variables\nquarto_render(\"program/cl_02.qmd\")\n\nKey transformations include:\n\nMissing Value Treatment: Replace missing value codes into na\nTop-coding Indicator: Generate variable indicating whether the value is topcoded (for example, income)\nInflation Adjustment: Adjust for inflation using appropriate price indices (we ususally need this step when we talk about price and income)\nRecode and Create Variables: Convert categorical values to numeric, group variables, or build new ones for analysis.\nSave What You Need: Keep only the cleaned, final set of variables for modeling or visualization\n\n\n\n3.4 Step 4: Sample Restrictions\nApply sample restrictions to create the final analytical dataset.\n\n# Apply sample restrictions\nquarto_render(\"program/cl_03.qmd\")\n\nPurpose: Implement inclusion/exclusion criteria to define the population of interest."
  },
  {
    "objectID": "data_organization.html#data-analyzing",
    "href": "data_organization.html#data-analyzing",
    "title": "Data Organization",
    "section": "4 Data Analyzing",
    "text": "4 Data Analyzing\n\n# Run your most basic analysis\nquarto_render(\"program/an_01.qmd\")"
  },
  {
    "objectID": "data_organization.html#data-processing-summary",
    "href": "data_organization.html#data-processing-summary",
    "title": "Data Organization",
    "section": "5 Data Processing Summary",
    "text": "5 Data Processing Summary\nThe data cleaning process follows a systematic approach:\n\nRaw Data Import ‚Üí Standardized file structure\nQuality Control ‚Üí Remove problematic allocated values\n\nFeature Engineering ‚Üí Create analysis variables\nSample Definition ‚Üí Apply research-specific restrictions"
  },
  {
    "objectID": "data_organization.html#file-structure",
    "href": "data_organization.html#file-structure",
    "title": "Data Organization",
    "section": "6 File Structure",
    "text": "6 File Structure\nproject/\n‚îú‚îÄ‚îÄ program/\n|   ‚îú‚îÄ‚îÄ _master.qmd  # Organize all the files under, set the main working directory\n‚îÇ   ‚îú‚îÄ‚îÄ cl_00.qmd    # Data import and renaming\n‚îÇ   ‚îú‚îÄ‚îÄ cl_01.qmd    # Clean allocated values\n‚îÇ   ‚îú‚îÄ‚îÄ cl_02.qmd    # Variable generation\n‚îÇ   ‚îú‚îÄ‚îÄ cl_03.qmd    # Sample restrictions\n|   ‚îú‚îÄ‚îÄ an_01.qmd    # This will be your basic model result\n|   ‚îî‚îÄ‚îÄ ...          # Other model that you want to build\n‚îú‚îÄ‚îÄ data/\n|   ‚îú‚îÄ‚îÄ source     # storage of the raw datasets\n|   ‚îî‚îÄ‚îÄ output     # storage of cleaned datasets\n‚îú‚îÄ‚îÄ log/           # storage of the results of data analysis \n‚îú‚îÄ‚îÄ graph/         # storage of graphs\n‚îî‚îÄ‚îÄ reference/     # reference of your research"
  },
  {
    "objectID": "data_organization.html#note",
    "href": "data_organization.html#note",
    "title": "Data Organization",
    "section": "7 üìå Note",
    "text": "7 üìå Note\n\nIf you‚Äôre working with more than one raw dataset, it‚Äôs good practice to create a separate cleaning script for each one. For example:\n\n\nquarto_render(\"program/cl_01_acs.qmd\")  # Load the acs data\n\nquarto_render(\"program/cl_01_cps.qmd\")  # Load the cps data\n\nIf the datasets need to be merged, write a dedicated merge script\n\nquarto_render(\"program/cl_04_merge.qmd\")\n\n\nYour _master script should do more than just run your code‚Äîit should also:\n\n\n\nDescribe the purpose of each cleaning and analysis file\nClearly label each stage of the workflow (e.g., cleaning, merging, modeling)\nHelp collaborators or reviewers understand the flow of your project at a glance"
  },
  {
    "objectID": "da_01.html",
    "href": "da_01.html",
    "title": "Identifying the Data",
    "section": "",
    "text": "The purpose of this page is to provide an introduction to the data access process when starting a research project. We‚Äôll walk through key steps and tools you‚Äôll need to locate and retrieve meaningful data from public sources like the U.S. Census.\nThis page covers:\n\nÔ∏è An introduction to census data: what it is and why we use it\n\nHow to identify and select the right dataset for your research question\n\nCommon issues you might encounter‚Äîand how to troubleshoot them\n\n\n\nCensus data are official statistics collected systematically by governments to capture demographic, economic, and social information about the population 1.\nCensus data is a powerful resource for researchers for several key reasons:\n\nLongitudinal Coverage\nOffers data across many years, making it ideal for studying trends over time.\nRelatively Clean Format\nPublic-use datasets are well-documented and often come pre-processed, saving time on data wrangling.\nRich, Multi-Dimensional Information\nIncludes demographic, economic, housing, and geographic variables‚Äîuseful across many research fields.\nRepresentative Sample\nDesigned to reflect the broader population, allowing for generalizable insights.\n\n\n\n\nCensus and survey data can be accessed from a variety of sources depending on your research needs:\n1.Public Agencies\n\nBureau of Labor Statistics (BLS)\nEmployment, wages, and unemployment rates from datasets such as:\n\nCPS (Current Population Survey)\n\nQCEW (Quarterly Census of Employment and Wages)\n\nLAUS (Local Area Unemployment Statistics)\n\nCensus Bureau\nPopulation, economic, housing data from:\n\nACS (American Community Survey)\n\nDecennial Census\n\nSIPP (Survey of Income and Program Participation)\n\nFederal Reserve\nFinancial and consumer data, including:\n\nFRED (Federal Reserve Economic Data)\n\nSCF (Survey of Consumer Finances)\n\n\n\nInternational Organizations\n\n\nGlobal development indicators and demographic data (e.g., World Bank, IMF, UN)\n\n\nState Government\n\n\nState-level data on employment, education, health, and more\n\n\nAcademic Data Archives\n\n\nIPUMS ‚Äì Integrated Public Use Microdata Series for harmonized census and survey data\n\n\nDirect Data Requests\n\n\nCustom requests through data repositories or agency contacts\n\n\n\n\n\nGeographical & Temporal Coverage\nNational, regional, state, or local levels\nAnnual, quarterly, monthly, or one-time collection\nVariable Definitions\nUnderstand how variables are constructed (e.g., age groups, inflation-adjusted prices)\nSurvey Design & Dataset Type\nCross-sectional vs.¬†longitudinal\nSample design: random, stratified, weighted\n\n\n\n\n\nIdentify the core variables needed for your research\nMatch the dataset‚Äôs geography and time coverage to your question\nLook for unique identifiers (e.g., FIPS codes) to support merging\nChoose datasets that are easy to access and compatible with your tools\n\nüìå When collecting data, it‚Äôs common to realize that no single dataset contains everything you need. A practical approach is to:\n\nAssess what‚Äôs possible with the data you already have.\nIdentify complementary datasets that can be merged ‚Äî this is especially helpful when working with aggregate-level data."
  },
  {
    "objectID": "da_01.html#purpose",
    "href": "da_01.html#purpose",
    "title": "Identifying the Data",
    "section": "",
    "text": "The purpose of this page is to provide an introduction to the data access process when starting a research project. We‚Äôll walk through key steps and tools you‚Äôll need to locate and retrieve meaningful data from public sources like the U.S. Census.\nThis page covers:\n\nÔ∏è An introduction to census data: what it is and why we use it\n\nHow to identify and select the right dataset for your research question\n\nCommon issues you might encounter‚Äîand how to troubleshoot them\n\n\n\nCensus data are official statistics collected systematically by governments to capture demographic, economic, and social information about the population 1.\nCensus data is a powerful resource for researchers for several key reasons:\n\nLongitudinal Coverage\nOffers data across many years, making it ideal for studying trends over time.\nRelatively Clean Format\nPublic-use datasets are well-documented and often come pre-processed, saving time on data wrangling.\nRich, Multi-Dimensional Information\nIncludes demographic, economic, housing, and geographic variables‚Äîuseful across many research fields.\nRepresentative Sample\nDesigned to reflect the broader population, allowing for generalizable insights.\n\n\n\n\nCensus and survey data can be accessed from a variety of sources depending on your research needs:\n1.Public Agencies\n\nBureau of Labor Statistics (BLS)\nEmployment, wages, and unemployment rates from datasets such as:\n\nCPS (Current Population Survey)\n\nQCEW (Quarterly Census of Employment and Wages)\n\nLAUS (Local Area Unemployment Statistics)\n\nCensus Bureau\nPopulation, economic, housing data from:\n\nACS (American Community Survey)\n\nDecennial Census\n\nSIPP (Survey of Income and Program Participation)\n\nFederal Reserve\nFinancial and consumer data, including:\n\nFRED (Federal Reserve Economic Data)\n\nSCF (Survey of Consumer Finances)\n\n\n\nInternational Organizations\n\n\nGlobal development indicators and demographic data (e.g., World Bank, IMF, UN)\n\n\nState Government\n\n\nState-level data on employment, education, health, and more\n\n\nAcademic Data Archives\n\n\nIPUMS ‚Äì Integrated Public Use Microdata Series for harmonized census and survey data\n\n\nDirect Data Requests\n\n\nCustom requests through data repositories or agency contacts\n\n\n\n\n\nGeographical & Temporal Coverage\nNational, regional, state, or local levels\nAnnual, quarterly, monthly, or one-time collection\nVariable Definitions\nUnderstand how variables are constructed (e.g., age groups, inflation-adjusted prices)\nSurvey Design & Dataset Type\nCross-sectional vs.¬†longitudinal\nSample design: random, stratified, weighted\n\n\n\n\n\nIdentify the core variables needed for your research\nMatch the dataset‚Äôs geography and time coverage to your question\nLook for unique identifiers (e.g., FIPS codes) to support merging\nChoose datasets that are easy to access and compatible with your tools\n\nüìå When collecting data, it‚Äôs common to realize that no single dataset contains everything you need. A practical approach is to:\n\nAssess what‚Äôs possible with the data you already have.\nIdentify complementary datasets that can be merged ‚Äî this is especially helpful when working with aggregate-level data."
  },
  {
    "objectID": "da_01.html#footnotes",
    "href": "da_01.html#footnotes",
    "title": "Identifying the Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat is Census?‚Ü©Ô∏é"
  },
  {
    "objectID": "da_02.html",
    "href": "da_02.html",
    "title": "Accessing the Data",
    "section": "",
    "text": "The purpose of this page is to introduce to you two ways which we can use to access the data, and lead you through the process of accessing the data with API. The two ways are :\n\ndirectly download\nUsing API and R packages\n\n\n\n\nData Format\nChoose the format that fits your software:\nUse .dta for Stata users, or .csv if you‚Äôre working in R, Python, or Excel.\nAllocated Values\nThese are values filled in by the agency when responses are missing‚Äîessentially informed guesses to complete the dataset.\nMissing Values\nCheck whether sufficient data was collected for your year of interest. Sparse responses may lead to smaller or less reliable samples.\nTime Coverage & Consistency\nSome datasets are only available for select years, and some variables are only asked in certain periods. Always confirm that your key variables are consistently available over time.\n\n\n\n\nAPI (Application Programming Interface):\nIn simple terms, an API is like a personal key that lets two programs connect and share information automatically. It allows you to access data from websites or services without manually downloading files.\nHere‚Äôs a general workflow when working with APIs:\n\nRegister for an API key\nMost data providers require you to sign up and get a unique key to use their API.\nInstall and load the necessary packages\nUse libraries designed to connect with the API (e.g., blsAPI, ipumsr, tidycensus).\nLoad the codebook or metadata\nUnderstand variable names, formats, and query structures.\nRun API calls to retrieve your data into R or another platform.\n\nüìå If there‚Äôs no official package for the API you want to use, check GitHub‚Äîmany developers create open-source packages or wrappers that simplify the process.\n\n\n\n# Set the library and install the packages\nlibrary(devtools)\nlibrary(blsR)\n\n# The introduction about this package: https://cran.r-project.org/web/packages/blsR/blsR.pdf\ndevtools::install_github(\"groditi/blsR\")\n\n# Acquire the API by registering through https://www.bls.gov/developers/home.htm\nbls_set_key(\"your personal API\")\n\ntest_series &lt;- get_series(series_id = \"series id you found\", \n                         start_year = 2016, \n                         end_year = 2024, \n                         api_key = bls_get_key())\n\nI work through the series number by https://data.bls.gov/PDQWeb/la, and we discover that the series number for each county is ‚ÄúFCN+Fips code+00000000‚Äù\n\n\n\n\n# Set the library and install the packages\nlibrary(censusapi)\nlibrary(tidycensus) \n\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\ncensus_api_key(\"your api key\", install = TRUE, overwrite = TRUE)\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\n#The main function to retrive the data here are https://cran.r-project.org/web/packages/tidycensus/tidycensus.pdf\n\n#to check the name of the variable\nv15 &lt;- load_variables(2015, \"acs5\", cache = TRUE)\nView(v15)\n\n# for example, if I want to know the age\n# here is the explanation of the codebook: https://data.census.gov/table/ACSDT5Y2022.B01001\n\nnc_acs_2015 &lt;- get_acs(geography = \"county\", \n              year = 2015,\n              variables = c(age = \"B01001A_003\"), \n              state = \"NC\",\n              survey = \"acs5\",\n              output = \"wide\")"
  },
  {
    "objectID": "da_02.html#purpose",
    "href": "da_02.html#purpose",
    "title": "Accessing the Data",
    "section": "",
    "text": "The purpose of this page is to introduce to you two ways which we can use to access the data, and lead you through the process of accessing the data with API. The two ways are :\n\ndirectly download\nUsing API and R packages\n\n\n\n\nData Format\nChoose the format that fits your software:\nUse .dta for Stata users, or .csv if you‚Äôre working in R, Python, or Excel.\nAllocated Values\nThese are values filled in by the agency when responses are missing‚Äîessentially informed guesses to complete the dataset.\nMissing Values\nCheck whether sufficient data was collected for your year of interest. Sparse responses may lead to smaller or less reliable samples.\nTime Coverage & Consistency\nSome datasets are only available for select years, and some variables are only asked in certain periods. Always confirm that your key variables are consistently available over time.\n\n\n\n\nAPI (Application Programming Interface):\nIn simple terms, an API is like a personal key that lets two programs connect and share information automatically. It allows you to access data from websites or services without manually downloading files.\nHere‚Äôs a general workflow when working with APIs:\n\nRegister for an API key\nMost data providers require you to sign up and get a unique key to use their API.\nInstall and load the necessary packages\nUse libraries designed to connect with the API (e.g., blsAPI, ipumsr, tidycensus).\nLoad the codebook or metadata\nUnderstand variable names, formats, and query structures.\nRun API calls to retrieve your data into R or another platform.\n\nüìå If there‚Äôs no official package for the API you want to use, check GitHub‚Äîmany developers create open-source packages or wrappers that simplify the process.\n\n\n\n# Set the library and install the packages\nlibrary(devtools)\nlibrary(blsR)\n\n# The introduction about this package: https://cran.r-project.org/web/packages/blsR/blsR.pdf\ndevtools::install_github(\"groditi/blsR\")\n\n# Acquire the API by registering through https://www.bls.gov/developers/home.htm\nbls_set_key(\"your personal API\")\n\ntest_series &lt;- get_series(series_id = \"series id you found\", \n                         start_year = 2016, \n                         end_year = 2024, \n                         api_key = bls_get_key())\n\nI work through the series number by https://data.bls.gov/PDQWeb/la, and we discover that the series number for each county is ‚ÄúFCN+Fips code+00000000‚Äù\n\n\n\n\n# Set the library and install the packages\nlibrary(censusapi)\nlibrary(tidycensus) \n\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\ncensus_api_key(\"your api key\", install = TRUE, overwrite = TRUE)\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\n#The main function to retrive the data here are https://cran.r-project.org/web/packages/tidycensus/tidycensus.pdf\n\n#to check the name of the variable\nv15 &lt;- load_variables(2015, \"acs5\", cache = TRUE)\nView(v15)\n\n# for example, if I want to know the age\n# here is the explanation of the codebook: https://data.census.gov/table/ACSDT5Y2022.B01001\n\nnc_acs_2015 &lt;- get_acs(geography = \"county\", \n              year = 2015,\n              variables = c(age = \"B01001A_003\"), \n              state = \"NC\",\n              survey = \"acs5\",\n              output = \"wide\")"
  },
  {
    "objectID": "da_02.html#troubleshooting",
    "href": "da_02.html#troubleshooting",
    "title": "Accessing the Data",
    "section": "2 üîå Troubleshooting",
    "text": "2 üîå Troubleshooting\n\n2.1 Issue 1: Choose Between 1-Year vs 5- Year ACS Estimate\nWhen we look into one of the most commonly used census data, ACS (American Community Survey), we will see that there are one year, 3 year and 5 year, which one we should go for?\nThe difference between these three can be divided into 3 dimension: geographical coverage, reliability, and recency [^1]\n\narea coverage : 1 year &lt; 3 year&lt; 5 year\n\n1 year census only cover area with population of 65,000+, 3 year covers areas with population of 20,000+, and 5 year covers all the areas\n\nreliability : 1 year &lt; 3 year &lt; 5 year\n\nThe intuition is that smaller sample sizes lead to higher margins of error. The margin of error reflects how well the sample represents the population‚Äîwhen the sample size is small, its representativeness decreases, increasing the likelihood of sampling error. [^2]\n\nrecency : 1 year &gt; 3 year &gt; 5 year\n\nA shorter sampling duration typically allows for earlier data publication.\nüìå In such cases, we typically rely on the 5-year Census estimates for greater data reliability. However, if a major policy change occurs, the 1-year estimates are better suited to capture its immediate impact. [^2]\n[^1] Using 1-Year or 5-Year American Community Survey Data\n[^2] U.S. Census Bureau American Community Survey (ACS) 1-Year vs.¬†5-Year Estimates"
  },
  {
    "objectID": "da_03.html",
    "href": "da_03.html",
    "title": "Working with Maps: Tigris and Geocoding",
    "section": "",
    "text": "This page introduces you to the basics of working with geographic data in R. Specifically, you‚Äôll learn:\n\nHow to use the tigris package to load map data\n\nHow to work with shapefiles and geographic boundaries\n\nHow to geocode locations"
  },
  {
    "objectID": "da_03.html#purpose",
    "href": "da_03.html#purpose",
    "title": "Working with Maps: Tigris and Geocoding",
    "section": "",
    "text": "This page introduces you to the basics of working with geographic data in R. Specifically, you‚Äôll learn:\n\nHow to use the tigris package to load map data\n\nHow to work with shapefiles and geographic boundaries\n\nHow to geocode locations"
  },
  {
    "objectID": "da_03.html#load-the-map",
    "href": "da_03.html#load-the-map",
    "title": "Working with Maps: Tigris and Geocoding",
    "section": "2 Load the map",
    "text": "2 Load the map\n\n2.1 Use of Tigris\nTigris is an R package that provides shapefiles and geographic boundary data down to the census tract and block group level. It‚Äôs especially useful for city-level analysis and any project involving geographic mapping.\nüß©Example For example, now I want to load the map of blockgroups in the county of Blacksburg in 2022\n\n#set up the package\nlibrary(tigris)\nlibrary(mapview)\n\n#load the map, with the fips code of VA and Blacksburg\nva_bg &lt;- block_groups(state = \"51\", county = \"121\", year = 2020)\n\n#to view the map \nmapview(va_bg, aplha = 1, alpha.regions = 0.1)\n\n\n\n2.2 working with shapefiles\nSome of the time, you may not be able to directly access the map by tigris. In that case, in most of the state and county website you are able to access their map in the .shp format, and you can read the file by\n\n#set up the library\nlibrary(sf)\n\nva_map &lt;- st_read(\"your working directory/the file.shp\")"
  },
  {
    "objectID": "da_03.html#geocode-locations",
    "href": "da_03.html#geocode-locations",
    "title": "Working with Maps: Tigris and Geocoding",
    "section": "3 Geocode Locations",
    "text": "3 Geocode Locations\nüåç Choosing a Geocoding API\nThere are two popular options when it comes to geocoding in R:\n\nüîé Google Geocoding API\n\nHigh accuracy and global coverage\n\nRequires an API key\n\nFree usage is limited ‚Äî charges may apply beyond the quota\n\nüÜì Tidygeocoder (OpenStreetMap/Census)\n\nFree and open-source\n\nNo API key required\n\nGreat for batch geocoding and educational use\n\nAccuracy may be lower compared to Google, especially for ambiguous or incomplete addresses\n\n\n\nüí° Use tidygeocoder for quick and cost-free geocoding. If you need higher precision or enterprise-level support, consider Google‚Äôs API.\n\nUsing Google\n\n#set up the library\nlibrary(ggmap)\nlibrary(ggplot2)\n\n# setup the library for the google api \n if(!requireNamespace(\"devtools\")) install.packages(\"devtools\")\n    devtools::install_github(\"dkahle/ggmap\", ref = \"tidyup\")\n\n    register_google(key = \"your api here\")\n\ngeocode(paste(stop_Field_2017$location[i], \"DC\"), timeout = 5000, verbose = TRUE,\n            source = \"google\", output = \"more\") -&gt; result\n\nUsing Tidygeocoder\n\nlibrary(tidygeocoder)\n\nxy_hut &lt;- geocode(\n  .tbl = data.frame(address = \"250 Drillfield Dr, Blacksburg, VA 24061\"),\n  address = address,\n  method = \"osm\",   \n  lat = latitude,\n  long = longitude,\n  full_results = TRUE,\n  verbose = TRUE\n)   \n\nhead(xy_hut)"
  },
  {
    "objectID": "example_1.html",
    "href": "example_1.html",
    "title": "example_1",
    "section": "",
    "text": "For this example, we‚Äôll walk through a simplified replication exercise based on the paper ‚ÄúDoes Compulsory School Attendance Affect Schooling and Earnings?‚Äù by Angrist and Krueger (1991). For this exercise, we‚Äôll clean a dataset from the ACS to prepare it for studying the relationship between years of education and income. Since this paper used the birth quarter as the instrument as the years of the education, we will go through the practice of cleaning out year of birth and quarter of birth as well.\n\n\nFor this task, we‚Äôll download data from IPUMS and split it by year. The purpose of this approach is to allow us to test the model‚Äôs significance separately for each individual year later on.\n\n# usa_00012 is the file name when you directly download from IPUMS\n\ndata_acs &lt;- read_csv(\"data/source/usa_00012.csv\")\n\n#Make sure the 'year' column is treated as a factor or numeric\ndata_acs$year &lt;- as.integer(data_acs$year)\n\n# Split the data into a list of datasets, one per year\n# Now data_by_year is a named list, e.g. data_by_year[[\"2001\"]], data_by_year[[\"2002\"]], etc.\n\ndata_by_year &lt;- split(data_acs, data_acs$year)\n\nfor (yr in names(data_by_year)) {\n  write.csv(data_by_year[[yr]], paste0(\"data/outcome/\", yr, \".csv\"), row.names = FALSE)\n}  \n\n\n\n\nIn this example, we need to work with 1980 data, which uses different set of allocated values compared to other years. Because of this, we handle the cleaning process for 1940 separately. This is also one of the main reasons we split the dataset by year in the first place‚Äî the more years of data we include, the more likely it is that coding differences or structural changes will occur across time.\n\n# Load 1980 data\ndata_1980 &lt;- read_csv(\"data/outcome/1980.csv\")\n\n#define the allocated values \nqvarlist &lt;- c(\"qage\", \"qmarst\", \"qsex\", \"qbpl\", \"qrace\", \"qeduc\", \n              \"qschool\", \"qclasswk\", \"qempstat\", \"quhrswor\", \n              \"qwkswork1\", \"qincwage\")\n\n\n# Process each variable in the list\nfor (i in qvarlist) {\n  # Drop observations where the variable value is greater than 3\n  # (equivalent to 'drop if `i'&gt;3')\n  data &lt;- data[data[[i]] &lt;= 3 | is.na(data[[i]]), ]\n  \n  # Drop the variable from the dataset\n  # (equivalent to 'drop `i'')\n  data &lt;- data[, !names(data) %in% i]\n}\n\n\n# Save cleaned 1980 data\nwrite_csv(data_1980, \"data/outcome/1980_i.csv\")\n\n\nyears &lt;- c(2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nqvarlist &lt;- c(\"qage\", \"qmarst\", \"qsex\", \"qbpl\", \"qrace\", \"qeduc\", \n              \"qschool\", \"qclasswk\", \"qempstat\", \"quhrswor\", \n              \"qwkswork1\", \"qincwage\")\n\n# Loop through each year (equivalent to 'foreach k in `r(numlist)'')\nfor (k in years) {\n  \n  # Load the data for current year (equivalent to 'use data/outcome/`k'.dta,clear')\n  data &lt;- read_dta(paste0(\"data/outcome/\", k, \".dta\"))\n  \n  # Process each variable in the list\n  for (i in qvarlist) {\n    # Drop observations where the variable value is greater than 3\n    # (equivalent to 'drop if `i'&gt;3')\n    data &lt;- data[data[[i]] &lt;= 3 | is.na(data[[i]]), ]\n    \n    # Drop the variable from the dataset\n    # (equivalent to 'drop `i'')\n    data &lt;- data[, !names(data) %in% i]\n  }\n  \n  # Save the processed data (equivalent to 'save data/outcome/`k'_i.dta,replace')\n  write_csv(data, paste0(\"data/outcome/\", k, \"_i.dta\"))\n}\n\n\n\n\nFor this dataset, our dependent variables will include: annual wage, weekly wage, and weeks worked in the previous year.\nThe key independent variables are: years of education, birth year, and birth quarter.\nAdditional demographic controls include: race, marital status, employment status, age group, working experience, and gender.\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\n# Loop through each year (equivalent to 'foreach i in `r(numlist)'')\nfor (i in years) {\n  \n  # Load the processed data (equivalent to 'use data/outcome/`i'_i,clear')\n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Replace incwage with missing value if &gt;= 999999\n  # (equivalent to 'replace incwage = . if incwage &gt;= 999999')\n  data$incwage[data$incwage &gt;= 999999] &lt;- NA\n  \n  # Save the data (equivalent to 'save data/outcome/`i'_i,replace')\n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n  # Print progress (optional)\n  cat(\"Processed incwage for year:\", i, \"\\n\")\n}\n\n\n\n\nFor this variable, each year can have different definition for the topcode.\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\") %&gt;%\n  mutate(topcoded = as.numeric(incwage &gt;= 75000 & incwage &lt; 999999 & !is.na(incwage)))\n\n# For 2005  \ndata_2005 &lt;- read_csv(\"data/outcome/2005_i.csv\") %&gt;%\n  mutate(topcoded = as.numeric(incwage &gt;= 175000 & incwage &lt; 999999 & !is.na(incwage)))\n...\n\n\n\n\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\")\n\ndata_1980$incwage &lt;- data_1980$incwage * 2.314\n\nwrite_csv(data_1980, \"data/outcome/1980_i.csv\")\n\ndata_2005 &lt;- read.csv(\"data/outcome/2005_i.csv\")\n\ndata_2005$incwage &lt;- data_2005$incwage * 0.853\n\nwrite.csv(data_2005, \"data/outcome/2005_i.csv\", row.names = FALSE)\n\n...\n\n\n\n\nAs mentioned earlier in the data access section, maintaining consistency of variables across years is important. This is a good example: the variable for years of education completed isn‚Äôt directly available in the ACS. Instead, we can use two closely related variables as proxies: highgrade(the highest grade of schooling, easy to clean for years of education, but not available after 1980) and educ(the education attainment, it is available for more years, but harder to clean).\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\")\n\n  data_1980$higrade[data_1980$higrade == 0] &lt;- NA\n  data_1980$higrade[data_1980$higrade %in% c(1, 2, 3)] &lt;- 0\n\n# Generate schooling variable (equivalent to 'g schooling = higrade')\n  data_1980$schooling &lt;- data_1980$higrade\n\n# Adjust schooling (equivalent to 'replace schooling = schooling - 3 if schooling&gt;3')\n  data_1980$schooling[data_1980$schooling &gt; 3 & !is.na(data_1980$schooling)] &lt;- \n  data_1980$schooling[data_1980$schooling &gt; 3 & !is.na(data_1980$schooling)] - 3\n\n# Generate experience (equivalent to 'g exp = age - schooling - 6')\n  data_1980$exp &lt;- data_1980$age - data_1980$schooling - 6\n\n# Replace negative experience with 0 \n  data_1980$exp[data_1980$exp &lt; 0 & !is.na(data_1980$exp)] &lt;- 0\n\n# Generate experience squared \n  data_1980$exp2 &lt;- data_1980$exp * data_1980$exp\n\n# Generate age squared \n  data_1980$age2 &lt;- data_1980$age * data_1980$age\n\n# Save the data (equivalent to 'save data/outcome/1980_i,replace')\n  write_csv(data_1980, \"data/outcome/1980_i.csv\")\n\n\nyears &lt;- c(2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_dta(paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n  # Generate schooling from educd \n  data$schooling &lt;- data$educd\n  \n  # Recode schooling values \n  schooling_map &lt;- c(\n  \"0\" = NA, \"1\" = NA, \"2\" = NA,\n  \"10\" = 2.5, \"21\" = 5.5, \"24\" = 7.5, \"30\" = 9,\n  \"40\" = 10, \"50\" = 11, \"61\" = 12, \"62\" = 12, \"63\" = 12, \"64\" = 12,\n  \"65\" = 13, \"71\" = 13, \"81\" = 14, \"101\" = 16, \"114\" = 18, \"115\" = 16, \"116\" = 16\n)\n  \n  data$schooling &lt;- schooling_map[as.character(data$educd)]\n  \n  data$exp &lt;- data$age - data$schooling - 6\n  \n  data$exp[data$exp &lt; 0 & !is.na(data$exp)] &lt;- 0\n  \n  data$exp2 &lt;- data$exp * data$exp\n  \n  data$age2 &lt;- data$age * data$age\n  \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n}\n\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\n# ****************************************************\n# Generate weekly wage\n# ****************************************************\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Recode wkswork1 \n  data$wkswork1[data$wkswork1 == 0] &lt;- NA\n  \n  # Generate weeks variable \n  data$weeks &lt;- data$wkswork1\n  \n  # Generate weekly wage \n  data$wagewk &lt;- data$incwage / data$weeks\n  \n  # Save the data \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n}\n\n# ****************************************************\n# Generate hourly wage\n# ****************************************************\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Recode uhrswork \n  data$uhrswork[data$uhrswork == 0] &lt;- NA\n  \n  # Generate topcoded hours indicator \n  data$topcoded_uhr &lt;- as.numeric(data$uhrswork == 99 & !is.na(data$uhrswork))\n  \n  # Generate hourly wage \n  data$wagehr &lt;- data$incwage / (data$weeks * data$uhrswork)\n  \n  # Save the data \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n}\n\nüí≠ Exercise: Can you write the code to generate each individual‚Äôs year of birth and quarter of birth based on the available variables?\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \ndata &lt;- read_dta(paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n# ****************************************************\n# Marital Status\n# ****************************************************\n  \n    # Generate married indicator \n    data$marr &lt;- as.numeric(data$marst == 1 | data$marst == 2)\n  \n  \n# ****************************************************\n# Have a child under age 5\n# ****************************************************\n  \n   # Generate child under 5 indicator \n  data$child5 &lt;- as.numeric(data$nchlt5 &gt; 0 & !is.na(data$nchlt5))\n  \n  ...\n  \n# ****************************************************\n# Keep the relevant variables\n# ****************************************************\n  \n  # Define variable lists \n  indvar &lt;- c(\"schooling\", \"exp\", \"exp2\", \"topcoded\")\n  \n  depvar &lt;- c(\"incwage\", \"wagewk\", \"wagehr\")\n  \n  othervar &lt;- c(\"year\", \"sample\", \"serial\", \"pernum\",\n                \"gq\", \"statefip\", \"age\", \"ageg\", \"age2\",\n                \"marr\", \"child5\", ...)\n  \n  pweight &lt;- c(\"perwt\")\n  \n  # Combine all variables to keep\n  vars_to_keep &lt;- c(depvar, indvar, othervar, pweight)\n  \n  # Method 1: Shortest - direct subsetting with error handling\n  data_subset &lt;- data[, intersect(vars_to_keep, names(data)), drop = FALSE]\n  \n  # Save the data (equivalent to 'save data/outcome/`i'_pre,replace')\n  write_csv(data_subset, paste0(\"data/outcome/\", i, \"_pre.csv\"))\n\n}\n\nüí≠ The code above shows how to create a few demographic variables. Now, try extending it by generating the remaining ones‚Äîthink about how you would code the rest based on the available data.\n\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_dta(paste0(\"data/outcome/\", i, \"_pre.dta\"))\n  \n  # if we limit the gender to male\n  data &lt;- data[data$male == 1 & !is.na(data$male), ]\n  \n  # keep only the individual with positive income\n  data &lt;- data[data$incwage &gt; 0 & !is.na(data$incwage), ]\n  \n  #touse the missing variables\n    data &lt;- data %&gt;%\n    mutate(\n      touse = complete.cases(select(., incwage, schooling, exp, exp2,\n                                   topcoded, perwt, ...))\n    ) %&gt;%\n    filter(touse)\n    \n  # If we want to have a unlinear model, you can post the log tranformation here \n\n  # Generate log wage \n  data$lnwage &lt;- log(data$incwage)\n    \n  write_dta(data, paste0(\"data/outcome/\", i, \"_final.dta\"))\n  \n}"
  },
  {
    "objectID": "example_1.html#example-1-replicating-ak-paper",
    "href": "example_1.html#example-1-replicating-ak-paper",
    "title": "example_1",
    "section": "",
    "text": "For this example, we‚Äôll walk through a simplified replication exercise based on the paper ‚ÄúDoes Compulsory School Attendance Affect Schooling and Earnings?‚Äù by Angrist and Krueger (1991). For this exercise, we‚Äôll clean a dataset from the ACS to prepare it for studying the relationship between years of education and income. Since this paper used the birth quarter as the instrument as the years of the education, we will go through the practice of cleaning out year of birth and quarter of birth as well.\n\n\nFor this task, we‚Äôll download data from IPUMS and split it by year. The purpose of this approach is to allow us to test the model‚Äôs significance separately for each individual year later on.\n\n# usa_00012 is the file name when you directly download from IPUMS\n\ndata_acs &lt;- read_csv(\"data/source/usa_00012.csv\")\n\n#Make sure the 'year' column is treated as a factor or numeric\ndata_acs$year &lt;- as.integer(data_acs$year)\n\n# Split the data into a list of datasets, one per year\n# Now data_by_year is a named list, e.g. data_by_year[[\"2001\"]], data_by_year[[\"2002\"]], etc.\n\ndata_by_year &lt;- split(data_acs, data_acs$year)\n\nfor (yr in names(data_by_year)) {\n  write.csv(data_by_year[[yr]], paste0(\"data/outcome/\", yr, \".csv\"), row.names = FALSE)\n}  \n\n\n\n\nIn this example, we need to work with 1980 data, which uses different set of allocated values compared to other years. Because of this, we handle the cleaning process for 1940 separately. This is also one of the main reasons we split the dataset by year in the first place‚Äî the more years of data we include, the more likely it is that coding differences or structural changes will occur across time.\n\n# Load 1980 data\ndata_1980 &lt;- read_csv(\"data/outcome/1980.csv\")\n\n#define the allocated values \nqvarlist &lt;- c(\"qage\", \"qmarst\", \"qsex\", \"qbpl\", \"qrace\", \"qeduc\", \n              \"qschool\", \"qclasswk\", \"qempstat\", \"quhrswor\", \n              \"qwkswork1\", \"qincwage\")\n\n\n# Process each variable in the list\nfor (i in qvarlist) {\n  # Drop observations where the variable value is greater than 3\n  # (equivalent to 'drop if `i'&gt;3')\n  data &lt;- data[data[[i]] &lt;= 3 | is.na(data[[i]]), ]\n  \n  # Drop the variable from the dataset\n  # (equivalent to 'drop `i'')\n  data &lt;- data[, !names(data) %in% i]\n}\n\n\n# Save cleaned 1980 data\nwrite_csv(data_1980, \"data/outcome/1980_i.csv\")\n\n\nyears &lt;- c(2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nqvarlist &lt;- c(\"qage\", \"qmarst\", \"qsex\", \"qbpl\", \"qrace\", \"qeduc\", \n              \"qschool\", \"qclasswk\", \"qempstat\", \"quhrswor\", \n              \"qwkswork1\", \"qincwage\")\n\n# Loop through each year (equivalent to 'foreach k in `r(numlist)'')\nfor (k in years) {\n  \n  # Load the data for current year (equivalent to 'use data/outcome/`k'.dta,clear')\n  data &lt;- read_dta(paste0(\"data/outcome/\", k, \".dta\"))\n  \n  # Process each variable in the list\n  for (i in qvarlist) {\n    # Drop observations where the variable value is greater than 3\n    # (equivalent to 'drop if `i'&gt;3')\n    data &lt;- data[data[[i]] &lt;= 3 | is.na(data[[i]]), ]\n    \n    # Drop the variable from the dataset\n    # (equivalent to 'drop `i'')\n    data &lt;- data[, !names(data) %in% i]\n  }\n  \n  # Save the processed data (equivalent to 'save data/outcome/`k'_i.dta,replace')\n  write_csv(data, paste0(\"data/outcome/\", k, \"_i.dta\"))\n}\n\n\n\n\nFor this dataset, our dependent variables will include: annual wage, weekly wage, and weeks worked in the previous year.\nThe key independent variables are: years of education, birth year, and birth quarter.\nAdditional demographic controls include: race, marital status, employment status, age group, working experience, and gender.\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\n# Loop through each year (equivalent to 'foreach i in `r(numlist)'')\nfor (i in years) {\n  \n  # Load the processed data (equivalent to 'use data/outcome/`i'_i,clear')\n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Replace incwage with missing value if &gt;= 999999\n  # (equivalent to 'replace incwage = . if incwage &gt;= 999999')\n  data$incwage[data$incwage &gt;= 999999] &lt;- NA\n  \n  # Save the data (equivalent to 'save data/outcome/`i'_i,replace')\n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n  # Print progress (optional)\n  cat(\"Processed incwage for year:\", i, \"\\n\")\n}\n\n\n\n\nFor this variable, each year can have different definition for the topcode.\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\") %&gt;%\n  mutate(topcoded = as.numeric(incwage &gt;= 75000 & incwage &lt; 999999 & !is.na(incwage)))\n\n# For 2005  \ndata_2005 &lt;- read_csv(\"data/outcome/2005_i.csv\") %&gt;%\n  mutate(topcoded = as.numeric(incwage &gt;= 175000 & incwage &lt; 999999 & !is.na(incwage)))\n...\n\n\n\n\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\")\n\ndata_1980$incwage &lt;- data_1980$incwage * 2.314\n\nwrite_csv(data_1980, \"data/outcome/1980_i.csv\")\n\ndata_2005 &lt;- read.csv(\"data/outcome/2005_i.csv\")\n\ndata_2005$incwage &lt;- data_2005$incwage * 0.853\n\nwrite.csv(data_2005, \"data/outcome/2005_i.csv\", row.names = FALSE)\n\n...\n\n\n\n\nAs mentioned earlier in the data access section, maintaining consistency of variables across years is important. This is a good example: the variable for years of education completed isn‚Äôt directly available in the ACS. Instead, we can use two closely related variables as proxies: highgrade(the highest grade of schooling, easy to clean for years of education, but not available after 1980) and educ(the education attainment, it is available for more years, but harder to clean).\n\ndata_1980 &lt;- read_csv(\"data/outcome/1980_i.csv\")\n\n  data_1980$higrade[data_1980$higrade == 0] &lt;- NA\n  data_1980$higrade[data_1980$higrade %in% c(1, 2, 3)] &lt;- 0\n\n# Generate schooling variable (equivalent to 'g schooling = higrade')\n  data_1980$schooling &lt;- data_1980$higrade\n\n# Adjust schooling (equivalent to 'replace schooling = schooling - 3 if schooling&gt;3')\n  data_1980$schooling[data_1980$schooling &gt; 3 & !is.na(data_1980$schooling)] &lt;- \n  data_1980$schooling[data_1980$schooling &gt; 3 & !is.na(data_1980$schooling)] - 3\n\n# Generate experience (equivalent to 'g exp = age - schooling - 6')\n  data_1980$exp &lt;- data_1980$age - data_1980$schooling - 6\n\n# Replace negative experience with 0 \n  data_1980$exp[data_1980$exp &lt; 0 & !is.na(data_1980$exp)] &lt;- 0\n\n# Generate experience squared \n  data_1980$exp2 &lt;- data_1980$exp * data_1980$exp\n\n# Generate age squared \n  data_1980$age2 &lt;- data_1980$age * data_1980$age\n\n# Save the data (equivalent to 'save data/outcome/1980_i,replace')\n  write_csv(data_1980, \"data/outcome/1980_i.csv\")\n\n\nyears &lt;- c(2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_dta(paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n  # Generate schooling from educd \n  data$schooling &lt;- data$educd\n  \n  # Recode schooling values \n  schooling_map &lt;- c(\n  \"0\" = NA, \"1\" = NA, \"2\" = NA,\n  \"10\" = 2.5, \"21\" = 5.5, \"24\" = 7.5, \"30\" = 9,\n  \"40\" = 10, \"50\" = 11, \"61\" = 12, \"62\" = 12, \"63\" = 12, \"64\" = 12,\n  \"65\" = 13, \"71\" = 13, \"81\" = 14, \"101\" = 16, \"114\" = 18, \"115\" = 16, \"116\" = 16\n)\n  \n  data$schooling &lt;- schooling_map[as.character(data$educd)]\n  \n  data$exp &lt;- data$age - data$schooling - 6\n  \n  data$exp[data$exp &lt; 0 & !is.na(data$exp)] &lt;- 0\n  \n  data$exp2 &lt;- data$exp * data$exp\n  \n  data$age2 &lt;- data$age * data$age\n  \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n}\n\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\n# ****************************************************\n# Generate weekly wage\n# ****************************************************\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Recode wkswork1 \n  data$wkswork1[data$wkswork1 == 0] &lt;- NA\n  \n  # Generate weeks variable \n  data$weeks &lt;- data$wkswork1\n  \n  # Generate weekly wage \n  data$wagewk &lt;- data$incwage / data$weeks\n  \n  # Save the data \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n}\n\n# ****************************************************\n# Generate hourly wage\n# ****************************************************\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_csv(paste0(\"data/outcome/\", i, \"_i.csv\"))\n  \n  # Recode uhrswork \n  data$uhrswork[data$uhrswork == 0] &lt;- NA\n  \n  # Generate topcoded hours indicator \n  data$topcoded_uhr &lt;- as.numeric(data$uhrswork == 99 & !is.na(data$uhrswork))\n  \n  # Generate hourly wage \n  data$wagehr &lt;- data$incwage / (data$weeks * data$uhrswork)\n  \n  # Save the data \n  write_csv(data, paste0(\"data/outcome/\", i, \"_i.csv\"))\n}\n\nüí≠ Exercise: Can you write the code to generate each individual‚Äôs year of birth and quarter of birth based on the available variables?\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \ndata &lt;- read_dta(paste0(\"data/outcome/\", i, \"_i.dta\"))\n  \n# ****************************************************\n# Marital Status\n# ****************************************************\n  \n    # Generate married indicator \n    data$marr &lt;- as.numeric(data$marst == 1 | data$marst == 2)\n  \n  \n# ****************************************************\n# Have a child under age 5\n# ****************************************************\n  \n   # Generate child under 5 indicator \n  data$child5 &lt;- as.numeric(data$nchlt5 &gt; 0 & !is.na(data$nchlt5))\n  \n  ...\n  \n# ****************************************************\n# Keep the relevant variables\n# ****************************************************\n  \n  # Define variable lists \n  indvar &lt;- c(\"schooling\", \"exp\", \"exp2\", \"topcoded\")\n  \n  depvar &lt;- c(\"incwage\", \"wagewk\", \"wagehr\")\n  \n  othervar &lt;- c(\"year\", \"sample\", \"serial\", \"pernum\",\n                \"gq\", \"statefip\", \"age\", \"ageg\", \"age2\",\n                \"marr\", \"child5\", ...)\n  \n  pweight &lt;- c(\"perwt\")\n  \n  # Combine all variables to keep\n  vars_to_keep &lt;- c(depvar, indvar, othervar, pweight)\n  \n  # Method 1: Shortest - direct subsetting with error handling\n  data_subset &lt;- data[, intersect(vars_to_keep, names(data)), drop = FALSE]\n  \n  # Save the data (equivalent to 'save data/outcome/`i'_pre,replace')\n  write_csv(data_subset, paste0(\"data/outcome/\", i, \"_pre.csv\"))\n\n}\n\nüí≠ The code above shows how to create a few demographic variables. Now, try extending it by generating the remaining ones‚Äîthink about how you would code the rest based on the available data.\n\n\n\n\n\nyears &lt;- c(1980, 2005, 2006, 2007, 2019, 2020, 2021, 2023)\n\nfor (i in years) {\n  \n  # Load the data \n  data &lt;- read_dta(paste0(\"data/outcome/\", i, \"_pre.dta\"))\n  \n  # if we limit the gender to male\n  data &lt;- data[data$male == 1 & !is.na(data$male), ]\n  \n  # keep only the individual with positive income\n  data &lt;- data[data$incwage &gt; 0 & !is.na(data$incwage), ]\n  \n  #touse the missing variables\n    data &lt;- data %&gt;%\n    mutate(\n      touse = complete.cases(select(., incwage, schooling, exp, exp2,\n                                   topcoded, perwt, ...))\n    ) %&gt;%\n    filter(touse)\n    \n  # If we want to have a unlinear model, you can post the log tranformation here \n\n  # Generate log wage \n  data$lnwage &lt;- log(data$incwage)\n    \n  write_dta(data, paste0(\"data/outcome/\", i, \"_final.dta\"))\n  \n}"
  },
  {
    "objectID": "example_2.html",
    "href": "example_2.html",
    "title": "example_2",
    "section": "",
    "text": "For this example, we are aiming to clean the fertility condition for the females, but these information are not straightforward In that case, we need to consider clean and identify the information by ourselves\n\nyears &lt;- c(1980, 1990)\n\nfor(year in years) {\n  # Load data\n  data_i &lt;- read_dta(paste0(\"data/outcome/\", year, \"_i.dta\"))\n  \n  # Sort by serial and birthyr\n  data_i %\n    arrange(serial, birthyr)\n  \n  # Generate birth order for children with mothers in household\n  data_i %\n    group_by(serial) %&gt;%\n    mutate(birth_order = ifelse(momloc != 0, row_number(), NA)) %&gt;%\n    ungroup()\n  \n  # Create indicator for child's sex (1 for male, 0 for female)\n  data_i %\n    mutate(male = ifelse(sex == 1, 1, 0))\n  \n  # Generate first child sex indicator (earliest birth year)\n  data_i %\n    group_by(serial) %&gt;%\n    mutate(\n      boy1st = case_when(\n        birth_order == 1 & male == 1 ~ 1,\n        birth_order == 1 & male == 0 ~ 0,\n        TRUE ~ NA_real_\n      )\n    ) %&gt;%\n    ungroup()\n  \n  # Generate second child sex indicator\n  data_i %\n    group_by(serial) %&gt;%\n    arrange(birth_order) %&gt;%\n    mutate(\n      boy2nd = case_when(\n        birth_order == 2 & male == 1 ~ 1,\n        birth_order == 2 & male == 0 ~ 0,\n        TRUE ~ NA_real_\n      )\n    ) %&gt;%\n    ungroup()\n  \n  # Save data\n  write_dta(data_i, paste0(\"data/outcome/\", year, \"_i.dta\"))\n}"
  },
  {
    "objectID": "example_2.html#example-2-to-merge-mothers-information",
    "href": "example_2.html#example-2-to-merge-mothers-information",
    "title": "example_2",
    "section": "",
    "text": "For this example, we are aiming to clean the fertility condition for the females, but these information are not straightforward In that case, we need to consider clean and identify the information by ourselves\n\nyears &lt;- c(1980, 1990)\n\nfor(year in years) {\n  # Load data\n  data_i &lt;- read_dta(paste0(\"data/outcome/\", year, \"_i.dta\"))\n  \n  # Sort by serial and birthyr\n  data_i %\n    arrange(serial, birthyr)\n  \n  # Generate birth order for children with mothers in household\n  data_i %\n    group_by(serial) %&gt;%\n    mutate(birth_order = ifelse(momloc != 0, row_number(), NA)) %&gt;%\n    ungroup()\n  \n  # Create indicator for child's sex (1 for male, 0 for female)\n  data_i %\n    mutate(male = ifelse(sex == 1, 1, 0))\n  \n  # Generate first child sex indicator (earliest birth year)\n  data_i %\n    group_by(serial) %&gt;%\n    mutate(\n      boy1st = case_when(\n        birth_order == 1 & male == 1 ~ 1,\n        birth_order == 1 & male == 0 ~ 0,\n        TRUE ~ NA_real_\n      )\n    ) %&gt;%\n    ungroup()\n  \n  # Generate second child sex indicator\n  data_i %\n    group_by(serial) %&gt;%\n    arrange(birth_order) %&gt;%\n    mutate(\n      boy2nd = case_when(\n        birth_order == 2 & male == 1 ~ 1,\n        birth_order == 2 & male == 0 ~ 0,\n        TRUE ~ NA_real_\n      )\n    ) %&gt;%\n    ungroup()\n  \n  # Save data\n  write_dta(data_i, paste0(\"data/outcome/\", year, \"_i.dta\"))\n}"
  },
  {
    "objectID": "example_2.html#identify-the-cases-that-the-number-of-children-in-the-household",
    "href": "example_2.html#identify-the-cases-that-the-number-of-children-in-the-household",
    "title": "example_2",
    "section": "2 Identify the cases that the number of children in the household",
    "text": "2 Identify the cases that the number of children in the household\n\nfor(year in years) {\n  # Load data\n  data_i &lt;- read_dta(paste0(\"data/outcome/\", year, \"_i.dta\"))\n  \n  # Generate child indicator and count children per household\n  data_i %\n    mutate(is_child = ifelse(relate == 3, 1, 0)) %&gt;%\n    group_by(serial) %&gt;%\n    mutate(nchild_hh = sum(is_child, na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  # Save data\n  write_dta(data_i, paste0(\"data/outcome/\", year, \"_i.dta\"))\n}"
  },
  {
    "objectID": "example_2.html#age-at-the-first-birth",
    "href": "example_2.html#age-at-the-first-birth",
    "title": "example_2",
    "section": "3 Age at the first birth",
    "text": "3 Age at the first birth\n\nfor(year in years) {\n  # Load data\n  data_i &lt;- read_dta(paste0(\"data/outcome/\", year, \"_i.dta\"))\n  \n  # Calculate age at first birth\n  data_i %\n    arrange(serial, birth_order) %&gt;%\n    group_by(serial) %&gt;%\n    mutate(\n      # Calculate age at first birth for first child\n      agefstm = case_when(\n        birth_order == 1 & !is.na(age_mom) & !is.na(age) ~ age_mom - age,\n        TRUE ~ NA_real_\n      )\n    ) %&gt;%\n    # Fill down the age at first birth for other children in same household\n    fill(agefstm, .direction = \"down\") %&gt;%\n    ungroup()\n  \n  # Save data\n  write_dta(data_i, paste0(\"data/outcome/\", year, \"_i.dta\"))\n}"
  },
  {
    "objectID": "example_2.html#example-3-geocode-the-locationstreet-address",
    "href": "example_2.html#example-3-geocode-the-locationstreet-address",
    "title": "example_2",
    "section": "4 Example 3: Geocode the location/street address",
    "text": "4 Example 3: Geocode the location/street address\nGeocoding is the process of converting addresses or place names into geographic coordinates (latitude and longitude). This tutorial demonstrates how to use R to geocode addresses using Google‚Äôs Geocoding API."
  },
  {
    "objectID": "example_2.html#setup-and-prerequisites",
    "href": "example_2.html#setup-and-prerequisites",
    "title": "example_2",
    "section": "5 Setup and Prerequisites",
    "text": "5 Setup and Prerequisites\n\n5.1 Step 1: Obtain Google Maps API Key\nBefore starting, you need to get a Google Maps API key:\n\nLog into your Google account\nAccess the Google Cloud Console: https://console.cloud.google.com/google/maps-apis/api-list\nSelect ‚ÄúCredentials‚Äù in the left sidebar\nCreate or copy your API key\nEnable the Geocoding API for your project\n\n\n\n5.2 Step 2: Load Required Libraries\n\n# Essential libraries for geocoding\nlibrary(ggmap)        # Main geocoding package\nlibrary(dplyr)        # Data manipulation\nlibrary(stringr)      # String operations\nlibrary(readr)        # Data import/export\nlibrary(purrr)        # Functional programming tools\n\n# Additional useful libraries\nlibrary(httr)         # HTTP requests\nlibrary(geosphere)    # Geographic calculations\nlibrary(ggplot2)      # Plotting\nlibrary(lubridate)    # Date/time operations\n\n\n\n5.3 Step 3: Install Development Version of ggmap\nThe CRAN version of ggmap may be outdated. Install the latest development version:\n\n# Install development version if needed\nif(!requireNamespace(\"devtools\")) install.packages(\"devtools\")\ndevtools::install_github(\"dkahle/ggmap\", ref = \"tidyup\")"
  },
  {
    "objectID": "example_2.html#setting-up-google-maps-api",
    "href": "example_2.html#setting-up-google-maps-api",
    "title": "example_2",
    "section": "6 Setting Up Google Maps API",
    "text": "6 Setting Up Google Maps API\n\n6.1 Register Your API Key\n\n# Register your Google Maps API key\n# Replace \"YOUR_API_KEY_HERE\" with your actual API key\nregister_google(key = \"YOUR_API_KEY_HERE\")\n\n# Verify registration\nhas_google_key()\n\n\n\n6.2 Test API Connection\n\n# Simple test geocoding\ntest_address &lt;- \"1600 Pennsylvania Avenue NW, Washington, DC\"\ntest_result &lt;- geocode(test_address, output = \"more\")\nprint(test_result)"
  },
  {
    "objectID": "example_2.html#basic-geocoding-examples",
    "href": "example_2.html#basic-geocoding-examples",
    "title": "example_2",
    "section": "7 Basic Geocoding Examples",
    "text": "7 Basic Geocoding Examples\n\n7.1 Single Address Geocoding\n\n# Geocode a single address\naddress &lt;- \"Harvard University, Cambridge, MA\"\n\n# Basic geocoding (returns lat/lon only)\ncoords_basic &lt;- geocode(address)\nprint(coords_basic)\n\n# Detailed geocoding (returns additional information)\ncoords_detailed &lt;- geocode(address, output = \"more\")\nprint(coords_detailed)\n\n\n\n7.2 Multiple Address Geocoding\n\n# Create sample data\naddresses %\n  mutate(\n    coords = map(location, ~ geocode(.x, output = \"more\")),\n    lat = map_dbl(coords, ~ .x$lat),\n    lon = map_dbl(coords, ~ .x$lon),\n    formatted_address = map_chr(coords, ~ .x$address)\n  ) %&gt;%\n  select(-coords)\n\nprint(addresses_geocoded)"
  },
  {
    "objectID": "example_2.html#advanced-geocoding-techniques",
    "href": "example_2.html#advanced-geocoding-techniques",
    "title": "example_2",
    "section": "8 Advanced Geocoding Techniques",
    "text": "8 Advanced Geocoding Techniques\n\n8.1 Batch Geocoding with Error Handling\n\n# Function for safe geocoding with error handling\nsafe_geocode  0) Sys.sleep(delay)\n  }\n  \n  # Combine results\n  geocoded_results &lt;- bind_rows(results)\n  return(bind_cols(data, geocoded_results))\n}\n\n\n\n8.2 Handling API Rate Limits\n\n# Function to geocode with automatic retries and rate limiting\ngeocode_with_retry &lt;- function(address, max_retries = 3, delay = 1) {\n  for(attempt in 1:max_retries) {\n    tryCatch({\n      result &lt;- geocode(address, output = \"more\")\n      if(!is.na(result$lat)) {\n        return(result)\n      }\n    }, error = function(e) {\n      if(attempt &lt; max_retries) {\n        cat(\"Attempt\", attempt, \"failed. Retrying in\", delay, \"seconds...\\n\")\n        Sys.sleep(delay)\n        delay &lt;- delay * 2  # Exponential backoff\n      }\n    })\n  }\n  \n  # Return NA if all attempts failed\n  return(data.frame(lon = NA, lat = NA, address = NA))\n}"
  },
  {
    "objectID": "example_2.html#practical-example-geocoding-from-your-original-code",
    "href": "example_2.html#practical-example-geocoding-from-your-original-code",
    "title": "example_2",
    "section": "9 Practical Example: Geocoding from Your Original Code",
    "text": "9 Practical Example: Geocoding from Your Original Code\n\n9.1 Reconstructed Example\n\n# Simulating your original data structure\n# Replace this with your actual data loading\nstop_Field_2017 &lt;- data.frame(\n  location = c(\n    \"Union Station\",\n    \"Capitol Hill\", \n    \"Georgetown\",\n    \"Dupont Circle\",\n    \"Adams Morgan\"\n  ),\n  stringsAsFactors = FALSE\n)\n\n# Geocoding loop (similar to your original approach)\ngeocoded_results &lt;- list()\n\nfor(i in 1:nrow(stop_Field_2017)) {\n  cat(\"Processing location\", i, \":\", stop_Field_2017$location[i], \"\\n\")\n  \n  # Create full address by adding DC\n  full_address &lt;- paste(stop_Field_2017$location[i], \"DC\")\n  \n  # Geocode with detailed output\n  result &lt;- geocode(\n    full_address, \n    timeout = 5000, \n    verbose = TRUE,\n    source = \"google\", \n    output = \"more\"\n  )\n  \n  # Store result\n  geocoded_results[[i]] &lt;- result\n  \n  # Add small delay to be respectful to API\n  Sys.sleep(0.2)\n}\n\n# Combine all results\nfinal_results &lt;- bind_rows(geocoded_results)\ncombined_data &lt;- bind_cols(stop_Field_2017, final_results)\n\nprint(combined_data)"
  },
  {
    "objectID": "example_2.html#quality-control-and-validation",
    "href": "example_2.html#quality-control-and-validation",
    "title": "example_2",
    "section": "10 Quality Control and Validation",
    "text": "10 Quality Control and Validation\n\n10.1 Check Geocoding Quality\n\n# Function to assess geocoding quality\nassess_geocoding_quality %\n    summarise(\n      total_addresses = n(),\n      successful_geocodes = sum(!is.na(lat)),\n      success_rate = mean(!is.na(lat)) * 100,\n      missing_coordinates = sum(is.na(lat))\n    )\n  \n  return(quality_summary)\n}\n\n# Apply quality assessment\nquality_report &lt;- assess_geocoding_quality(combined_data)\nprint(quality_report)\n\n\n\n10.2 Visualize Results\n\n# Plot geocoded locations\nif(any(!is.na(combined_data$lat))) {\n  ggplot(combined_data, aes(x = lon, y = lat)) +\n    geom_point(color = \"red\", size = 3, alpha = 0.7) +\n    geom_text(aes(label = location), vjust = -1, size = 3) +\n    labs(\n      title = \"Geocoded Locations\",\n      x = \"Longitude\",\n      y = \"Latitude\"\n    ) +\n    theme_minimal()\n}"
  },
  {
    "objectID": "example_2.html#best-practices-and-tips",
    "href": "example_2.html#best-practices-and-tips",
    "title": "example_2",
    "section": "11 Best Practices and Tips",
    "text": "11 Best Practices and Tips\n\n11.1 1. API Key Security\n\nNever hardcode API keys in scripts\nUse environment variables: Sys.getenv(\"GOOGLE_MAPS_API_KEY\")\nStore keys in .Renviron file\n\n\n\n11.2 2. Rate Limiting\n\nGoogle allows 50 requests per second for geocoding\nAdd delays between requests for large batches\nConsider using Sys.sleep() between geocoding calls\n\n\n\n11.3 3. Address Formatting\n\nBe consistent with address formats\nInclude city, state, and country when possible\nClean addresses before geocoding (remove special characters)\n\n\n\n11.4 4. Error Handling\n\nAlways include try-catch blocks for large batches\nSave intermediate results to avoid losing progress\nLog failed geocoding attempts for manual review\n\n\n\n11.5 5. Data Validation\n\nCheck for missing coordinates\nValidate coordinates are within expected geographic bounds\nReview geocoded addresses against original inputs"
  },
  {
    "objectID": "example_2.html#troubleshooting-common-issues",
    "href": "example_2.html#troubleshooting-common-issues",
    "title": "example_2",
    "section": "12 Troubleshooting Common Issues",
    "text": "12 Troubleshooting Common Issues\n\n12.1 Issue 1: API Key Not Working\n# Check if API key is registered\nhas_google_key()\n\n# Re-register if needed\nregister_google(key = \"your-new-key\")\n\n\n12.2 Issue 2: Quota Exceeded\n# Implement exponential backoff\nSys.sleep(2^attempt)\n\n\n12.3 Issue 3: Poor Geocoding Results\n# Try different address formats\n# Add more geographic context (city, state, country)"
  },
  {
    "objectID": "example_2.html#summary",
    "href": "example_2.html#summary",
    "title": "example_2",
    "section": "13 Summary",
    "text": "13 Summary\nThis tutorial covered: - Setting up Google Maps API for geocoding - Basic and advanced geocoding techniques - Error handling and rate limiting - Quality control and validation - Best practices for production use\nRemember to always respect API terms of service and implement appropriate rate limiting for large-scale geocoding projects."
  },
  {
    "objectID": "R_shiny Tutorial_students.html",
    "href": "R_shiny Tutorial_students.html",
    "title": "R_shiny",
    "section": "",
    "text": "The Purpose of this slide is to help you with understanding the functions of R shiny and show you some examples about how to using it.\n\n\n\nlibrary(shiny)\n\n\n\n\nThe components of R shiny can be divided into 3 parts:\n\nUI (User Interface) :\n\nThis is the part the user sees and interacts with. It includes:\n\nInputs : Elements like sliders, text boxes, and dropdowns that collect user input.\nOutputs: Elements like plots, tables, and text that display results.\n\n\n\nServer : This is the backend logic that connects inputs and outputs.\nReactivity : This is the system that enables automatic updates. When a user changes an input, the output updates automatically.\nObserver : Observers perform side effects in response to input changes but do not return values. For example, they can be used to display a notification or log a message when a user enters information.\n\n\nui &lt;- fluidPage(\n  # your layout and UI elements here\n)\n\nserver &lt;- function(input, output, session) {\n  # your reactive logic here\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\ntext\n\ntextInput(\"name of the input\", \"the message you shown to the user: \")\n\nselect\n\nselectInput(\"name of the input\", \"\")\n\nslide\n\n# If you only want to slide an input \n\nsliderInput(\"slider1\", label = h3(\"Slider\"), min = 0, \n        max = 100, value = 50)\n\n# if you want to slide an range\nui &lt;- fluidPage(\n  fluidRow(\n        sliderInput(\"slider1\", label = h3(\"Slider\"), min = 0,  \n        max = 100, value = 50)\n    ),\n        sliderInput(\"slider2\", label = h3(\"Slider Range\"), min = 0, \n        max = 100, value = c(40, 60))\n  )\n\nYou can explore more input in theR shiny control widgets page\n\n\n\nüìå In Shiny, each output function you define in the UI is typically linked to a corresponding render function in the server. Together, they work to generate and display the output on the user‚Äôs side.\nTable Output\n\nDT::dataTableOutput(\"table\")\nDT::renderDataTable(DT::datatable{\n  ...\n})\n\nüìå We usually use DT for table output instead of tableOutput, because DT allows use to use\nPlot Output\n\nPlotOutput()\nrenderPlot({\n  ...\n})"
  },
  {
    "objectID": "R_shiny Tutorial_students.html#r-shiny-tutorial",
    "href": "R_shiny Tutorial_students.html#r-shiny-tutorial",
    "title": "R_shiny",
    "section": "",
    "text": "The Purpose of this slide is to help you with understanding the functions of R shiny and show you some examples about how to using it.\n\n\n\nlibrary(shiny)\n\n\n\n\nThe components of R shiny can be divided into 3 parts:\n\nUI (User Interface) :\n\nThis is the part the user sees and interacts with. It includes:\n\nInputs : Elements like sliders, text boxes, and dropdowns that collect user input.\nOutputs: Elements like plots, tables, and text that display results.\n\n\n\nServer : This is the backend logic that connects inputs and outputs.\nReactivity : This is the system that enables automatic updates. When a user changes an input, the output updates automatically.\nObserver : Observers perform side effects in response to input changes but do not return values. For example, they can be used to display a notification or log a message when a user enters information.\n\n\nui &lt;- fluidPage(\n  # your layout and UI elements here\n)\n\nserver &lt;- function(input, output, session) {\n  # your reactive logic here\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\ntext\n\ntextInput(\"name of the input\", \"the message you shown to the user: \")\n\nselect\n\nselectInput(\"name of the input\", \"\")\n\nslide\n\n# If you only want to slide an input \n\nsliderInput(\"slider1\", label = h3(\"Slider\"), min = 0, \n        max = 100, value = 50)\n\n# if you want to slide an range\nui &lt;- fluidPage(\n  fluidRow(\n        sliderInput(\"slider1\", label = h3(\"Slider\"), min = 0,  \n        max = 100, value = 50)\n    ),\n        sliderInput(\"slider2\", label = h3(\"Slider Range\"), min = 0, \n        max = 100, value = c(40, 60))\n  )\n\nYou can explore more input in theR shiny control widgets page\n\n\n\nüìå In Shiny, each output function you define in the UI is typically linked to a corresponding render function in the server. Together, they work to generate and display the output on the user‚Äôs side.\nTable Output\n\nDT::dataTableOutput(\"table\")\nDT::renderDataTable(DT::datatable{\n  ...\n})\n\nüìå We usually use DT for table output instead of tableOutput, because DT allows use to use\nPlot Output\n\nPlotOutput()\nrenderPlot({\n  ...\n})"
  },
  {
    "objectID": "R_shiny Tutorial_students.html#example",
    "href": "R_shiny Tutorial_students.html#example",
    "title": "R_shiny",
    "section": "2 Example",
    "text": "2 Example\n\n2.1 How to interact with the map\nFor this example, I want to show you how to use the R shiny to generate an interactive map and combine this function with the other packages, such as tigris.\n\n2.1.1 Set up\n\n#set up the tigris\noptions(tigris_use_cache = TRUE)\n\n\n#load the library\nlibrary(shiny)\nlibrary(tigris)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(censusapi)\nlibrary(tidycensus)\n\n\n\n\n2.2 Load the map\n\nva_counties &lt;- counties(state = \"VA\", year = 2022)\n\nThe education_data One of the column is the population of people with a high school diploma, and another column is the population who have a middle school diploma\n\n# Set the library and install the packages\n\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\ncensus_api_key(\"Your own API\", install = TRUE, overwrite = TRUE)\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\n#The main function to retrive the data here are https://cran.r-project.org/web/packages/tidycensus/tidycensus.pdf\n\n#to check the name of the variable\nv19 &lt;- load_variables(2019, \"acs5\", cache = TRUE)\nView(v19)\n\n# for example, if I want to know the age\n# here is the explanation of the codebook: https://data.census.gov/table/ACSDT5Y2022.B01001\n\nva_2019 &lt;- get_acs(geography = \"county\", \n              year = 2019,\n              variables = c(total_population_25 = \"B15003_001\", high_school = \"B15003_017\", middle_school = \"B15003_012\"), \n              state = \"VA\",\n              survey = \"acs5\",\n              output = \"wide\")\n\neducation_data &lt;- va_2019 %&gt;%\n  rename(total_population_25 = total_population_25E, \n         high_school = high_schoolE,\n         middle_school = middle_schoolE) \n\n  #change the county name to make it match the one on the map\neducation_data$county_name &lt;- gsub(\", Virginia$\", \"\", education_data$NAME)\n\nDefine the UI\n\nui &lt;- fluidPage(\n  titlePanel(\"Virginia Counties - Education Levels\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      h4(\"Controls\"),\n      selectInput(\"education_type\", \n                 \"Show Education Level:\",\n                 choices = list(\"High School Diploma\" = \"high_school\",\n                               \"Middle School Diploma\" = \"middle_school\")),\n      \n      br(),\n      h4(\"County Info\"),\n      textOutput(\"clicked_county\"),\n      textOutput(\"education_value\")\n    ),\n    \n    mainPanel(\n      leafletOutput(\"map\", height = \"500px\")\n    )\n  )\n)\n\n\n2.2.1 Define the Server\n\nserver &lt;- function(input, output, session) {\n  \n  # Create the map\n  output$map &lt;- renderLeaflet({\n    leaflet(va_counties) %&gt;%\n      addTiles() %&gt;%\n      setView(lng = -78.6569, lat = 37.4316, zoom = 7) %&gt;%\n      addPolygons(\n        fillColor = \"lightblue\",\n        weight = 2,\n        opacity = 1,\n        color = \"white\",\n        fillOpacity = 0.7,\n        layerId = ~NAME,  # This allows us to identify which county was clicked\n        popup = ~paste(\"County:\", NAME),\n        highlight = highlightOptions(\n          weight = 5,\n          color = \"#666\",\n          fillOpacity = 0.7,\n          bringToFront = TRUE\n        )\n      )\n  })\n  \n  observe({\n    # Get the selected education data\n\n    selected_values &lt;- education_data[[input$education_type]]\n    \n    # Create a simple color function\n    colors &lt;- ifelse(va_counties$GEOID %in% education_data$GEOID, \"red\", \"lightblue\")\n    \n    # Update the map\n    leafletProxy(\"map\") %&gt;%\n      clearShapes() %&gt;%\n      addPolygons(\n        data = va_counties,\n        fillColor = colors,\n        weight = 2,\n        opacity = 1,\n        color = \"white\",\n        fillOpacity = 0.7,\n        layerId = ~NAME,\n        popup = ~paste(\"County:\", NAME),\n        highlight = highlightOptions(\n          weight = 5,\n          color = \"#666\",\n          fillOpacity = 0.7,\n          bringToFront = TRUE\n        )\n      )\n  })\n  \n  # Show info when county is clicked\n  output$clicked_county &lt;- renderText({\n    click &lt;- input$map_shape_click\n    if (is.null(click)) {\n      \"Click on a county\"\n    } else {\n      paste(\"Selected:\", click$id)\n    }\n  })\n  \n   output$education_value &lt;- renderText({\n    click &lt;- input$map_shape_click\n    if (is.null(click)) {\n      \"No data to show\"\n    } else {\n      # Find the education data for this county\n  # First get the GEOID of the clicked county\n    clicked_county &lt;- va_counties[va_counties$NAME == click$id, ]\n    county_geoid &lt;- clicked_county$GEOID\n\n# Then match using GEOID\ncounty_data &lt;- education_data[education_data$GEOID == county_geoid, ]      \n      if (nrow(county_data) &gt; 0) {\n        value &lt;- county_data[[input$education_type]]\n        paste(input$education_type, \":\", value)\n      } else {\n        \"No education data for this county\"\n      }\n    }\n  })\n}\n\nRun the application\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "R_shiny Tutorial_students.html#exercise",
    "href": "R_shiny Tutorial_students.html#exercise",
    "title": "R_shiny",
    "section": "3 Exercise",
    "text": "3 Exercise\n\n3.1 1. Change the State from Virginia (VA) to North Carolina (NC)\n\n\n3.2 2. Display Both the Count and Percentage of the Selected Education Level\nWhen a user clicks on a county, display:\nthe number of people with the selected education level\nthe percentage of people with the selected education level among the total population aged 25 and older in that county\n\n\n3.3 3. Change the Map Fill Color from Red to Blue in the observe() block\n\n\n3.4 Answers:"
  }
]